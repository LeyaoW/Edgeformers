{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f795fb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "from transformers import BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1105533",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery  # python client for Google BigQuery\n",
    "import pandas as pd  # python library for [tabular] data analysis and manipulation\n",
    "import numpy as np  # python's mathematical library for n-dimensional arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a770f755",
   "metadata": {},
   "source": [
    "## download data with Google Cloud SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60910802",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"google_cloud.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f86e954b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make connection\n",
    "client = bigquery.Client()  # create BigQuery client to communicate with the database\n",
    "\n",
    "# create a reference to stackoverflow dataset\n",
    "dataset_ref = client.dataset(\"stackoverflow\", project=\"bigquery-public-data\")\n",
    "\n",
    "# get the dataset resource\n",
    "dataset = client.get_dataset(dataset_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b987753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the data\n",
    "tables = list(client.list_tables(dataset))  # list all the tables in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95973278",
   "metadata": {},
   "source": [
    "#### Only focus on comments table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9147ea64",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Construct a reference to the \"posts_answers\" table\n",
    "comments_table_ref = dataset_ref.table(\"comments\")\n",
    "\n",
    "# API request - fetch the table\n",
    "comments_table = client.get_table(comments_table_ref)\n",
    "\n",
    "# Preview the first five lines of the \"posts_answers\" table\n",
    "#data_df = client.list_rows(comments_table, max_results=2000000).to_dataframe()\n",
    "data_df = client.list_rows(comments_table).to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "05c8077d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to json\n",
    "#json_result = data_df.to_json(orient=\"records\")\n",
    "#with open('stackoverflow/data.json','w') as f:\n",
    "#    json.dump(json_result, f, indent = 6)\n",
    "    \n",
    "data_df.to_json('stackoverflow/data.json', orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4db3a4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape data with SQL (no permission)\n",
    "######################### (no permission) #########################\n",
    "query = \"\"\"\n",
    "        SELECT text, post_id, user_id\n",
    "        FROM `bigquery-public-data.stackoverflow.comments` \n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "364a99e1",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "Forbidden",
     "evalue": "403 POST https://bigquery.googleapis.com/bigquery/v2/projects/proud-sweep-344117/jobs?prettyPrint=false: Access Denied: Project proud-sweep-344117: User does not have bigquery.jobs.create permission in project proud-sweep-344117.\n\nLocation: None\nJob ID: 623dbeb3-7592-40e8-b33d-d3352028ad9f\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mForbidden\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-aa53207687ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msafe_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbigquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQueryJobConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaximum_bytes_billed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mquery_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msafe_config\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Your code goes here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# API request - run the query, and return a pandas DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared/data2/bowenj4/miniconda3/envs/graphformer/lib/python3.6/site-packages/google/cloud/bigquery/client.py\u001b[0m in \u001b[0;36mquery\u001b[0;34m(self, query, job_config, job_id, job_id_prefix, location, project, retry, timeout, job_retry)\u001b[0m\n\u001b[1;32m   3389\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mquery_job\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3391\u001b[0;31m         \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdo_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3392\u001b[0m         \u001b[0;31m# The future might be in a failed state now, but if it's\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3393\u001b[0m         \u001b[0;31m# unrecoverable, we'll find out when we ask for it's result, at which\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared/data2/bowenj4/miniconda3/envs/graphformer/lib/python3.6/site-packages/google/cloud/bigquery/client.py\u001b[0m in \u001b[0;36mdo_query\u001b[0;34m()\u001b[0m\n\u001b[1;32m   3366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3367\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3368\u001b[0;31m                 \u001b[0mquery_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3369\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mcore_exceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConflict\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcreate_exc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3370\u001b[0m                 \u001b[0;31m# The thought is if someone is providing their own job IDs and they get\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared/data2/bowenj4/miniconda3/envs/graphformer/lib/python3.6/site-packages/google/cloud/bigquery/job/query.py\u001b[0m in \u001b[0;36m_begin\u001b[0;34m(self, client, retry, timeout)\u001b[0m\n\u001b[1;32m   1295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQueryJob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1298\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGoogleAPICallError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m             exc.message = _EXCEPTION_FOOTER_TEMPLATE.format(\n",
      "\u001b[0;32m/shared/data2/bowenj4/miniconda3/envs/graphformer/lib/python3.6/site-packages/google/cloud/bigquery/job/base.py\u001b[0m in \u001b[0;36m_begin\u001b[0;34m(self, client, retry, timeout)\u001b[0m\n\u001b[1;32m    516\u001b[0m             \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_api_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m             \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m         )\n\u001b[1;32m    520\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_properties\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_response\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared/data2/bowenj4/miniconda3/envs/graphformer/lib/python3.6/site-packages/google/cloud/bigquery/client.py\u001b[0m in \u001b[0;36m_call_api\u001b[0;34m(self, retry, span_name, span_attributes, job_ref, headers, **kwargs)\u001b[0m\n\u001b[1;32m    780\u001b[0m                 \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspan_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspan_attributes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             ):\n\u001b[0;32m--> 782\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared/data2/bowenj4/miniconda3/envs/graphformer/lib/python3.6/site-packages/google/api_core/retry.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m                 \u001b[0msleep_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_deadline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m                 \u001b[0mon_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_error\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m             )\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared/data2/bowenj4/miniconda3/envs/graphformer/lib/python3.6/site-packages/google/api_core/retry.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, deadline, on_error)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msleep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msleep_generator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared/data2/bowenj4/miniconda3/envs/graphformer/lib/python3.6/site-packages/google/cloud/_http/__init__.py\u001b[0m in \u001b[0;36mapi_request\u001b[0;34m(self, method, path, query_params, data, content_type, headers, api_base_url, api_version, expect_json, _target_object, timeout)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_http_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mexpect_json\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mForbidden\u001b[0m: 403 POST https://bigquery.googleapis.com/bigquery/v2/projects/proud-sweep-344117/jobs?prettyPrint=false: Access Denied: Project proud-sweep-344117: User does not have bigquery.jobs.create permission in project proud-sweep-344117.\n\nLocation: None\nJob ID: 623dbeb3-7592-40e8-b33d-d3352028ad9f\n"
     ]
    }
   ],
   "source": [
    "# Set up the query (cancel the query if it would use too much of \n",
    "# your quota, with the limit set to 1 GB)\n",
    "######################### (no permission) #########################\n",
    "\n",
    "safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\n",
    "query_job = client.query(query, job_config=safe_config) # Your code goes here\n",
    "\n",
    "# API request - run the query, and return a pandas DataFrame\n",
    "comments_results = query_job.to_dataframe() # Your code goes here\n",
    "\n",
    "# Preview results\n",
    "print(comments_results.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5c9c90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3baddf92",
   "metadata": {},
   "source": [
    "# read review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "667e1f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8a3445a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'stackoverflow'\n",
    "data_name = 'data'\n",
    "output_dir='xxx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ba7e9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read raw data\n",
    "with open(f'{dataset}/{data_name}.json') as f:\n",
    "    data = json.load(f)\n",
    "random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a6ff75f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83160601"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "931180b5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 69179198,\n",
       " 'text': 'Please also take some effort to format your post and your code properly. I applied some basic indentation to your code.',\n",
       " 'creation_date': 1480978297137,\n",
       " 'post_id': 40984235,\n",
       " 'user_id': 721644.0,\n",
       " 'user_display_name': None,\n",
       " 'score': 1}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49f8fd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text processing function\n",
    "def text_process(text):\n",
    "    p_text = ' '.join(text.split('\\r\\n'))\n",
    "    p_text = ' '.join(text.split('\\n\\r'))\n",
    "    p_text = ' '.join(text.split('\\n'))\n",
    "    p_text = ' '.join(p_text.split('\\t'))\n",
    "    p_text = ' '.join(p_text.split('\\rm'))\n",
    "    p_text = ' '.join(p_text.split('\\r'))\n",
    "    p_text = ''.join(p_text.split('$'))\n",
    "    p_text = ''.join(p_text.split('*'))\n",
    "\n",
    "    return p_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "139bfffc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83160601/83160601 [02:25<00:00, 570566.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {0: 69307730, 2: 2471553, 1: 9326709, 3: 945067, 6: 123687, 5: 205822, 4: 427859, 11: 21220, 19: 4742, 55: 439, 10: 25103, 31: 1702, 17: 6338, 24: 2818, 7: 82440, 72: 205, 23: 3086, 8: 51639, 15: 7846, 13: 13290, 48: 569, 12: 16424, 9: 36934, 26: 2385, 88: 116, 14: 10899, 32: 1457, 25: 2570, 18: 5265, 16: 6823, 28: 1872, 51: 510, 39: 857, 53: 450, 80: 142, 22: 3352, 49: 500, 70: 200, 63: 291, 40: 796, 35: 1119, 38: 944, 41: 872, 71: 242, 69: 217, 37: 1066, 30: 1602, 215: 9, 162: 28, 21: 3918, 76: 180, 27: 2257, 50: 483, 33: 1323, 57: 409, 29: 1886, 60: 298, 114: 65, 36: 1100, 52: 438, 127: 45, 47: 692, 159: 23, 42: 794, 101: 81, 146: 27, 163: 24, 373: 3, 34: 1277, 106: 64, 46: 576, 86: 116, 140: 42, 79: 161, 20: 4043, 372: 3, 289: 1, 73: 196, 58: 378, 431: 2, 111: 48, 85: 125, 296: 8, 64: 298, 56: 391, 44: 679, 54: 420, 82: 152, 43: 716, 45: 668, 102: 63, 183: 22, 649: 1, 105: 62, 1331: 1, 161: 26, 94: 90, 125: 45, 148: 31, 268: 8, 89: 121, 153: 24, 110: 58, 251: 7, 83: 153, 66: 253, 155: 22, 196: 19, 221: 13, 62: 329, 157: 29, 169: 19, 192: 21, 59: 342, 90: 132, 68: 257, 130: 47, 61: 351, 223: 13, 134: 36, 74: 190, 120: 54, 172: 21, 84: 141, 97: 84, 126: 38, 133: 44, 226: 10, 187: 12, 91: 122, 167: 21, 143: 36, 77: 204, 93: 102, 75: 165, 81: 187, 151: 25, 118: 61, 119: 52, 107: 65, 177: 16, 67: 267, 317: 5, 87: 155, 113: 69, 132: 34, 243: 9, 286: 3, 112: 66, 324: 5, 181: 18, 219: 11, 216: 12, 65: 273, 245: 6, 247: 10, 78: 179, 104: 64, 246: 7, 253: 12, 158: 30, 103: 57, 302: 8, 549: 2, 258: 9, 124: 44, 229: 7, 244: 6, 96: 101, 233: 9, 423: 2, 138: 34, 213: 9, 147: 35, 121: 39, 255: 7, 234: 7, 122: 56, 191: 15, 224: 16, 95: 119, 220: 8, 285: 6, 231: 9, 227: 5, 176: 22, 189: 16, 279: 3, 131: 39, 123: 44, 790: 1, 145: 29, 184: 13, 98: 73, 142: 35, 482: 2, 154: 24, 144: 49, 128: 41, 99: 72, 179: 21, 422: 1, 139: 35, 117: 54, 141: 31, 264: 9, 188: 8, 300: 2, 150: 22, 222: 6, 210: 10, 199: 6, 173: 22, 272: 7, 250: 3, 170: 15, 115: 54, 232: 8, 850: 1, 310: 1, 180: 12, 211: 14, 100: 65, 92: 106, 225: 9, 137: 35, 203: 14, 186: 6, 331: 2, 377: 2, 339: 3, 509: 2, 109: 53, 116: 51, 135: 37, 108: 51, 275: 8, 156: 23, 149: 24, 209: 10, 165: 23, 344: 3, 190: 15, 288: 6, 392: 1, 235: 7, 257: 6, 166: 25, 178: 19, 254: 4, 174: 23, 660: 1, 262: 6, 440: 4, 198: 12, 152: 24, 647: 1, 129: 29, 263: 8, 136: 19, 347: 3, 175: 16, 237: 10, 450: 2, 462: 1, 559: 1, 514: 1, 395: 2, 205: 9, 481: 2, 201: 7, 414: 2, 212: 16, 390: 3, 479: 1, 298: 2, 304: 5, 202: 10, 160: 16, 168: 19, 217: 8, 218: 13, 281: 3, 388: 4, 182: 17, 241: 4, 363: 1, 386: 1, 194: 10, 260: 2, 378: 4, 330: 3, 274: 2, 564: 1, 266: 8, 248: 6, 607: 2, 371: 2, 236: 6, 290: 4, 204: 17, 551: 1, 295: 2, 261: 5, 361: 3, 273: 3, 164: 16, 252: 3, 321: 2, 287: 3, 305: 2, 329: 1, 1139: 1, 403: 1, 758: 1, 456: 1, 259: 6, 193: 14, 442: 2, 346: 3, 336: 1, 239: 10, 200: 9, 309: 2, 171: 25, 397: 3, 516: 1, 208: 10, 366: 1, 415: 2, 206: 10, 185: 12, 249: 2, 489: 2, 539: 1, 283: 6, 195: 8, 240: 7, 685: 1, 379: 2, 282: 3, 576: 1, 360: 3, 350: 2, 417: 1, 526: 1, 197: 15, 265: 4, 276: 4, 270: 5, 428: 1, 267: 5, 293: 4, 435: 1, 355: 6, 382: 3, 714: 1, 256: 4, 319: 4, 384: 2, 629: 2, 327: 3, 228: 3, 349: 3, 620: 1, 418: 1, 308: 4, 381: 2, 566: 1, 508: 1, 335: 4, 353: 2, 544: 1, 671: 1, 545: 1, 741: 1, 334: 3, 527: 2, 394: 1, 364: 2, 369: 2, 532: 1, 367: 2, 207: 5, 416: 1, 458: 1, 269: 3, 882: 1, 391: 4, 278: 3, 325: 3, 420: 2, 410: 1, 405: 2, 214: 8, 230: 3, 581: 1, 359: 1, 338: 1, 351: 2, 280: 6, 376: 3, 491: 1, 407: 1, 448: 1, 314: 3, 291: 4, 750: 1, 601: 1, 1690: 1, 464: 1, 354: 2, 374: 1, 427: 1, 393: 1, 389: 2, 332: 1, 466: 1, 734: 1, 284: 4, 342: 4, 470: 1, 860: 1, 303: 2, 402: 4, 322: 1, 555: 1, 238: 3, 399: 2, 297: 1, 674: 1, 430: 1, 341: 2, 1625: 1, 556: 1, 693: 1, 547: 1, 316: 1, 340: 3, 776: 1, 623: 1, 292: 1, 432: 1, 447: 1, 683: 1, 345: 3, 328: 3, 529: 1, 294: 1, 320: 1, 271: 2, 907: 1, 1371: 1, 443: 1, 311: 4, 956: 1, 318: 1, 575: 2, 513: 1, 426: 1, 689: 1, 531: 1, 469: 1, 596: 1, 438: 1, 521: 2, 584: 1, 421: 2, 646: 1, 455: 1, 277: 3, 501: 1, 485: 1, 680: 1, 525: 1, 1351: 1, 326: 2, 661: 2, 676: 1, 868: 1, 433: 1, 782: 2, 1190: 1, 579: 1, 459: 1, 751: 1, 375: 1, 543: 1, 608: 1, 1091: 1, 439: 1, 679: 1, 368: 1, 546: 1, 419: 1, 301: 1, 444: 1, 490: 1, 463: 2, 362: 1, 497: 1, 541: 1, 496: 1, 476: 1, 506: 1, 534: 1, 487: 1, 401: 1, 413: 2, 242: 1, 468: 1, 538: 1, 505: 1, 437: 1, 313: 1, 472: 1, 503: 1, 306: 1})\n",
      "83160601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## rate distribution\n",
    "\n",
    "rate_dict = defaultdict(int)\n",
    "all_rates = 0\n",
    "\n",
    "for d in tqdm(data):\n",
    "    rate_dict[d['score']] += 1\n",
    "    all_rates += 1\n",
    "    \n",
    "print(rate_dict)\n",
    "print(all_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c0eff9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83160601/83160601 [43:57<00:00, 31535.34it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of blank review:58\n",
      "Number of user:3468581, Number of item:27738624\n",
      "user_pos_reviews.len:3468581,user_neg_reviews.len:0\n",
      "item_pos_reviews.len:27738624,item_neg_reviews.len:0\n",
      "user.avg.review:23.97539541385944, item.avg.review:2.998007435408476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## user/item statistics\n",
    "### we see 5 score as positive edge(review), 1-4 as negative ones.\n",
    "### user_pos_reviews/user_neg_reviews: key<-userID, value<-list(reviews)\n",
    "### item_pos_reviews/item_neg_reviews: key<-productID, value<-list(reviews)\n",
    "### user_reviews_dict_save/item_reviews_dict_save: key<-userID/productID, value<-list(tuple(reviews,p/n))\n",
    "\n",
    "user_pos_reviews = defaultdict(list)\n",
    "user_neg_reviews = defaultdict(list)\n",
    "item_pos_reviews = defaultdict(list)\n",
    "item_neg_reviews = defaultdict(list)\n",
    "user_set = set()\n",
    "item_set = set()\n",
    "\n",
    "user_reviews_dict_save = defaultdict(dict)\n",
    "item_reviews_dict_save = defaultdict(dict)\n",
    "\n",
    "blank_review_cnt = 0\n",
    "\n",
    "for d in tqdm(data):\n",
    "    if 'text' not in d or d['text'] == '' or d['text'] == None:\n",
    "        blank_review_cnt += 1\n",
    "        continue\n",
    "    \n",
    "    text = text_process(d['text'])\n",
    "    user_set.add(d['user_id'])\n",
    "    item_set.add(d['post_id'])\n",
    "    if d['score'] >=0 :\n",
    "        user_pos_reviews[d['user_id']].append(text)\n",
    "        item_pos_reviews[d['post_id']].append(text)\n",
    "        \n",
    "        if d['post_id'] not in user_reviews_dict_save[d['user_id']]:\n",
    "            user_reviews_dict_save[d['user_id']][d['post_id']] = [text,d['creation_date']]\n",
    "        else:\n",
    "            user_reviews_dict_save[d['user_id']][d['post_id']][0] += text\n",
    "            user_reviews_dict_save[d['user_id']][d['post_id']][1] = max(user_reviews_dict_save[d['user_id']][d['post_id']][1], d['creation_date'])\n",
    "\n",
    "        if d['post_id'] not in item_reviews_dict_save[d['post_id']]:\n",
    "            item_reviews_dict_save[d['post_id']][d['user_id']] = [text,d['creation_date']]\n",
    "        else:\n",
    "            item_reviews_dict_save[d['post_id']][d['user_id']][0] += text\n",
    "            item_reviews_dict_save[d['post_id']][d['user_id']][1] = max(item_reviews_dict_save[d['post_id']][d['user_id']][1], d['creation_date'])            \n",
    "    else:\n",
    "        raise ValueError('Error!')\n",
    "        \n",
    "print(f'Number of blank review:{blank_review_cnt}')\n",
    "print(f'Number of user:{len(user_set)}, Number of item:{len(item_set)}')\n",
    "print(f'user_pos_reviews.len:{len(user_pos_reviews)},user_neg_reviews.len:{len(user_neg_reviews)}')\n",
    "print(f'item_pos_reviews.len:{len(item_pos_reviews)},item_neg_reviews.len:{len(item_neg_reviews)}')\n",
    "print(f'user.avg.review:{all_rates/len(user_set)}, item.avg.review:{all_rates/len(item_set)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7393583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of blank review:58\n",
      "Number of user:3468581, Number of item:27738624\n",
      "user_pos_reviews.len:3468581,user_neg_reviews.len:0\n",
      "item_pos_reviews.len:27738624,item_neg_reviews.len:0\n",
      "user.avg.review:23.97539541385944, item.avg.review:2.998007435408476\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of blank review:{blank_review_cnt}')\n",
    "print(f'Number of user:{len(user_set)}, Number of item:{len(item_set)}')\n",
    "print(f'user_pos_reviews.len:{len(user_pos_reviews)},user_neg_reviews.len:{len(user_neg_reviews)}')\n",
    "print(f'item_pos_reviews.len:{len(item_pos_reviews)},item_neg_reviews.len:{len(item_neg_reviews)}')\n",
    "print(f'user.avg.review:{all_rates/len(user_set)}, item.avg.review:{all_rates/len(item_set)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85418c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3468581/3468581 [06:05<00:00, 9490.74it/s]  \n",
      "100%|██████████| 27738624/27738624 [03:09<00:00, 146286.50it/s]\n"
     ]
    }
   ],
   "source": [
    "# transfer to the original format\n",
    "\n",
    "user_reviews_dict = {}\n",
    "item_reviews_dict = {}\n",
    "\n",
    "for i in tqdm(user_reviews_dict_save):\n",
    "    user_reviews_dict[i] = [(j, user_reviews_dict_save[i][j][0], user_reviews_dict_save[i][j][1]) for j in user_reviews_dict_save[i]]\n",
    "\n",
    "for i in tqdm(item_reviews_dict_save):\n",
    "    item_reviews_dict[i] = [(j, item_reviews_dict_save[i][j][0], item_reviews_dict_save[i][j][1]) for j in item_reviews_dict_save[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc1be69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump\n",
    "pickle.dump(user_reviews_dict,open(f'{output_dir}/user_reviews_dict.pkl','wb'))\n",
    "pickle.dump(item_reviews_dict,open(f'{output_dir}/item_reviews_dict.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce2aefe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7304f3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27738624/27738624 [01:30<00:00, 307698.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of above line user:138816\n",
      "Number of above line item:26027\n",
      "None cnt:3172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## filter\n",
    "\n",
    "line = 10\n",
    "none_cnt = 0\n",
    "\n",
    "item_reviews_cnt_dict = {}\n",
    "item_reviews_filtered_dict = {}\n",
    "user_reviews_filtered_dict = defaultdict(list)\n",
    "\n",
    "for i in tqdm(item_reviews_dict):\n",
    "    ## add it into cnt dict\n",
    "    if i in item_reviews_cnt_dict:\n",
    "        raise ValueError('stop')\n",
    "    item_reviews_cnt_dict[i] = len(item_reviews_dict[i])\n",
    "\n",
    "    ## reorder it\n",
    "    item_reviews_dict[i].sort(key=lambda x:x[2])\n",
    "    \n",
    "    ## statistics for line\n",
    "    if len(item_reviews_dict[i]) >= line:\n",
    "        item_reviews_filtered_dict[i] = deepcopy(item_reviews_dict[i])\n",
    "        for r in item_reviews_filtered_dict[i]:\n",
    "            if r[0] == None:\n",
    "                none_cnt += 1\n",
    "            else:\n",
    "                # user_reviews_filtered_dict[int(r[0])].append((r[0],i,r[2],1))\n",
    "                user_reviews_filtered_dict[int(r[0])].append((i,r[1],r[2]))\n",
    "\n",
    "print(f'Number of above line user:{len(user_reviews_filtered_dict)}')\n",
    "print(f'Number of above line item:{len(item_reviews_filtered_dict)}')\n",
    "print(f'None cnt:{none_cnt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2011d967",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 138816/138816 [00:00<00:00, 1360352.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average degree of filtered user:2.304446173351775\n",
      "Average degree of filtered item:12.290851807738118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# print filtered statistics\n",
    "\n",
    "edge_cnt = 0\n",
    "for u in tqdm(user_reviews_filtered_dict):\n",
    "    edge_cnt += len(user_reviews_filtered_dict[u])\n",
    "\n",
    "print(f'Average degree of filtered user:{edge_cnt / len(user_reviews_filtered_dict)}')\n",
    "print(f'Average degree of filtered item:{edge_cnt / len(item_reviews_filtered_dict)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "691d2a4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26027/26027 [00:01<00:00, 17180.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of user appearing in train_set:103295 or 103295\n",
      "Train/Val/Test size:217822,31652,73592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## split train/val/test as 7:1:2 or 8:1:1\n",
    "### user_pos_reviews/user_neg_reviews: key<-userID, value<-list(reviews)\n",
    "### item_pos_reviews/item_neg_reviews: key<-productID, value<-list(reviews)\n",
    "### train_user_neighbor: key<-userID, value<-list(tuple(reviews,p/n))\n",
    "### train_item_neighbor: key<-userID, value<-list(tuple(reviews,p/n))\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "train_tuples = []\n",
    "val_tuples = []\n",
    "test_tuples = []\n",
    "train_user_set = set()\n",
    "user_id2idx = {}\n",
    "item_id2idx = {}\n",
    "train_user_neighbor = defaultdict(list)\n",
    "train_item_neighbor = defaultdict(list)\n",
    "\n",
    "for iid in tqdm(item_reviews_filtered_dict):\n",
    "    if iid not in item_id2idx:\n",
    "        item_id2idx[iid] = len(item_id2idx)\n",
    "    \n",
    "    for i in range(int(len(item_reviews_filtered_dict[iid])*0.7)):\n",
    "        train_tuples.append((iid,item_reviews_filtered_dict[iid][i]))\n",
    "        train_user_set.add(item_reviews_filtered_dict[iid][i][0])\n",
    "\n",
    "        # add to user_id2idx\n",
    "        if item_reviews_filtered_dict[iid][i][0] not in user_id2idx:\n",
    "            user_id2idx[item_reviews_filtered_dict[iid][i][0]] = len(user_id2idx)\n",
    "\n",
    "        # add to train_user_neighbor/train_item_neighbor        \n",
    "        train_item_neighbor[iid].append(item_reviews_filtered_dict[iid][i])\n",
    "        train_user_neighbor[item_reviews_filtered_dict[iid][i][0]].append((iid,item_reviews_filtered_dict[iid][i][1],\n",
    "                                                                            item_reviews_filtered_dict[iid][i][2]))\n",
    "        \n",
    "    for i in range(int(len(item_reviews_filtered_dict[iid])*0.7),int(len(item_reviews_filtered_dict[iid])*0.8)):\n",
    "        val_tuples.append((iid,item_reviews_filtered_dict[iid][i]))\n",
    "\n",
    "    for i in range(int(len(item_reviews_filtered_dict[iid])*0.8),len(item_reviews_filtered_dict[iid])):\n",
    "        test_tuples.append((iid,item_reviews_filtered_dict[iid][i]))\n",
    "        \n",
    "print(f'Number of user appearing in train_set:{len(train_user_set)} or {len(user_id2idx)}')\n",
    "print(f'Train/Val/Test size:{len(train_tuples)},{len(val_tuples)},{len(test_tuples)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f526721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of item appearing in train_set:26027\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of item appearing in train_set:{len(item_id2idx)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8274368b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "408bef47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40544215,\n",
       " (100297.0,\n",
       "  'In those 7 years, you have posted 5 questions on Meta (that still survive today), plus another [20 on Meta.SE](https://meta.stackexchange.com/users/150595/jigar-joshi?tab=activity&sort=posts). Do you plan to become more active?',\n",
       "  1478855118773))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tuples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28ce6ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b6e668bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 217822/217822 [01:01<00:00, 3521.10it/s]\n"
     ]
    }
   ],
   "source": [
    "# generate and save train file\n",
    "## user pos neighbor: 2, user neg neighbor: 0\n",
    "## item pos neighbor: 5, item neg neighbor: 0\n",
    "\n",
    "upos = 2\n",
    "ipos = 5\n",
    "uneg = 0\n",
    "ineg = 0\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "with open(f'{output_dir}/{dataset}/train.tsv','w') as fout:\n",
    "    for d in tqdm(train_tuples):\n",
    "\n",
    "        # prepare sample pool for user and item\n",
    "        item_pos_pool = set(deepcopy(train_item_neighbor[d[0]]))\n",
    "        user_pos_pool = set(deepcopy(train_user_neighbor[d[1][0]]))\n",
    "        \n",
    "        # remove train sample from neighbor file\n",
    "        item_pos_pool.remove(d[1])\n",
    "        user_pos_pool.remove((d[0],d[1][1],d[1][2]))\n",
    "        \n",
    "        user_pos_pool = list(user_pos_pool)\n",
    "        item_pos_pool = list(item_pos_pool)\n",
    "        random.shuffle(user_pos_pool)\n",
    "        random.shuffle(item_pos_pool)\n",
    "        \n",
    "        user_neg_pool = []\n",
    "        item_neg_pool = []\n",
    "        \n",
    "        # sample for user\n",
    "        if len(user_pos_pool) >= upos:\n",
    "            user_pos_samples = user_pos_pool[:upos]\n",
    "        else:\n",
    "            user_pos_samples = user_pos_pool + [(-1,'')] * (upos-len(user_pos_pool))\n",
    "        \n",
    "        if len(user_neg_pool) >= uneg:\n",
    "            user_neg_samples = user_neg_pool[:uneg]\n",
    "        else:\n",
    "            user_neg_samples = user_neg_pool + [(-1,'')] * (uneg-len(user_neg_pool))\n",
    "        \n",
    "        # sample for item\n",
    "        if len(item_pos_pool) >= ipos:\n",
    "            item_pos_samples = item_pos_pool[:ipos]\n",
    "        else:\n",
    "            item_pos_samples = item_pos_pool + [(-1,'')] * (ipos-len(item_pos_pool))\n",
    "        \n",
    "        if len(item_neg_pool) >= ineg:\n",
    "            item_neg_samples = item_neg_pool[:ineg]\n",
    "        else:\n",
    "            item_neg_samples = item_neg_pool + [(-1,'')] * (ineg-len(item_neg_pool))\n",
    "        \n",
    "        # prepare for writing file\n",
    "        user_pos_text = '\\t'.join([up[1] for up in user_pos_samples])\n",
    "        user_pos_neighbor = '\\t'.join([str(item_id2idx[up[0]]) if up[0] != -1 else str(-1) for up in user_pos_samples])\n",
    "        user_neg_text = '\\t'.join([un[1] for un in user_neg_samples])\n",
    "        user_neg_neighbor = '\\t'.join([str(item_id2idx[un[0]]) if un[0] != -1 else str(-1) for un in user_neg_samples])\n",
    "        \n",
    "        item_pos_text = '\\t'.join([ip[1] for ip in item_pos_samples])\n",
    "        item_pos_neighbor = '\\t'.join([str(user_id2idx[ip[0]]) if ip[0] != -1 else str(-1) for ip in item_pos_samples])\n",
    "        item_neg_text = '\\t'.join([inn[1] for inn in item_neg_samples])\n",
    "        item_neg_neighbor = '\\t'.join([str(user_id2idx[inn[0]]) if inn[0] != -1 else str(-1) for inn in item_neg_samples])\n",
    "        \n",
    "        user_line = str(user_id2idx[d[1][0]]) + '\\*\\*' + user_pos_text + '\\*\\*' + user_neg_text + '\\*\\*' + user_pos_neighbor + '\\*\\*' + user_neg_neighbor\n",
    "        item_line = str(item_id2idx[d[0]]) + '\\*\\*' + item_pos_text + '\\*\\*' + item_neg_text + '\\*\\*' + item_pos_neighbor + '\\*\\*' + item_neg_neighbor\n",
    "        \n",
    "        # fout.write(user_line+'\\$\\$'+item_line+'\\$\\$'+str(d[1][3])+'\\n')\n",
    "        fout.write(item_line+'\\$\\$'+user_line+'\\$\\$'+str(1)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a1af48f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31652/31652 [00:07<00:00, 4451.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Valid Dev Edges:20246 | Total:31652\n"
     ]
    }
   ],
   "source": [
    "# generate and save val file (make sure to delete items that are not in train set)\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "valid_dev_edges = 0\n",
    "\n",
    "with open(f'{output_dir}/{dataset}/val.tsv','w') as fout:\n",
    "    for d in tqdm(val_tuples):\n",
    "        # if item not in train item set, continue\n",
    "        if d[1][0] not in train_user_set:\n",
    "            continue\n",
    "\n",
    "        # counting\n",
    "        valid_dev_edges += 1\n",
    "\n",
    "        # prepare sample pool for user and item\n",
    "        user_neg_pool = []\n",
    "        item_neg_pool = []\n",
    "        \n",
    "        item_pos_pool = deepcopy(train_item_neighbor[d[0]])\n",
    "        user_pos_pool = deepcopy(train_user_neighbor[d[1][0]])\n",
    "        \n",
    "        random.shuffle(user_pos_pool)\n",
    "        random.shuffle(item_pos_pool)\n",
    "        \n",
    "        # sample for user\n",
    "        if len(user_pos_pool) >= upos:\n",
    "            user_pos_samples = user_pos_pool[:upos]\n",
    "        else:\n",
    "            user_pos_samples = user_pos_pool + [(-1,'')] * (upos-len(user_pos_pool))\n",
    "        \n",
    "        if len(user_neg_pool) >= uneg:\n",
    "            user_neg_samples = user_neg_pool[:uneg]\n",
    "        else:\n",
    "            user_neg_samples = user_neg_pool + [(-1,'')] * (uneg-len(user_neg_pool))\n",
    "        \n",
    "        # sample for item\n",
    "        if len(item_pos_pool) >= ipos:\n",
    "            item_pos_samples = item_pos_pool[:ipos]\n",
    "        else:\n",
    "            item_pos_samples = item_pos_pool + [(-1,'')] * (ipos-len(item_pos_pool))\n",
    "        \n",
    "        if len(item_neg_pool) >= ineg:\n",
    "            item_neg_samples = item_neg_pool[:ineg]\n",
    "        else:\n",
    "            item_neg_samples = item_neg_pool + [(-1,'')] * (ineg-len(item_neg_pool))\n",
    "        \n",
    "        # prepare for writing file\n",
    "        user_pos_text = '\\t'.join([up[1] for up in user_pos_samples])\n",
    "        user_pos_neighbor = '\\t'.join([str(item_id2idx[up[0]]) if up[0] != -1 else str(-1) for up in user_pos_samples])\n",
    "        user_neg_text = '\\t'.join([un[1] for un in user_neg_samples])\n",
    "        user_neg_neighbor = '\\t'.join([str(item_id2idx[un[0]]) if un[0] != -1 else str(-1) for un in user_neg_samples])\n",
    "        \n",
    "        item_pos_text = '\\t'.join([ip[1] for ip in item_pos_samples])\n",
    "        item_pos_neighbor = '\\t'.join([str(user_id2idx[ip[0]]) if ip[0] != -1 else str(-1) for ip in item_pos_samples])\n",
    "        item_neg_text = '\\t'.join([inn[1] for inn in item_neg_samples])\n",
    "        item_neg_neighbor = '\\t'.join([str(user_id2idx[inn[0]]) if inn[0] != -1 else str(-1) for inn in item_neg_samples])\n",
    "        \n",
    "        user_line = str(user_id2idx[d[1][0]]) + '\\*\\*' + user_pos_text + '\\*\\*' + user_neg_text + '\\*\\*' + user_pos_neighbor + '\\*\\*' + user_neg_neighbor\n",
    "        item_line = str(item_id2idx[d[0]]) + '\\*\\*' + item_pos_text + '\\*\\*' + item_neg_text + '\\*\\*' + item_pos_neighbor + '\\*\\*' + item_neg_neighbor\n",
    "        \n",
    "        #fout.write(user_line+'\\$\\$'+item_line+'\\$\\$'+str(d[1][3])+'\\n')\n",
    "        fout.write(item_line+'\\$\\$'+user_line+'\\$\\$'+str(1)+'\\n')\n",
    "        \n",
    "print(f'Number of Valid Dev Edges:{valid_dev_edges} | Total:{len(val_tuples)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b14bc30d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73592/73592 [00:14<00:00, 5042.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Valid Test Edges:43589 | Total:73592\n"
     ]
    }
   ],
   "source": [
    "# generate and save test file (make sure to delete items that are not in train set)\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "valid_test_edges = 0\n",
    "\n",
    "with open(f'{output_dir}/{dataset}/test.tsv','w') as fout:\n",
    "    for d in tqdm(test_tuples):\n",
    "        # if item not in train item set, continue\n",
    "        if d[1][0] not in train_user_set:\n",
    "            continue\n",
    "\n",
    "        # counting\n",
    "        valid_test_edges += 1\n",
    "\n",
    "        # prepare sample pool for user and item\n",
    "        user_neg_pool = []\n",
    "        item_neg_pool = []\n",
    "        \n",
    "        item_pos_pool = deepcopy(train_item_neighbor[d[0]])\n",
    "        user_pos_pool = deepcopy(train_user_neighbor[d[1][0]])\n",
    "        \n",
    "        random.shuffle(user_pos_pool)\n",
    "        random.shuffle(item_pos_pool)\n",
    "        \n",
    "        # sample for user\n",
    "        if len(user_pos_pool) >= upos:\n",
    "            user_pos_samples = user_pos_pool[:upos]\n",
    "        else:\n",
    "            user_pos_samples = user_pos_pool + [(-1,'')] * (upos-len(user_pos_pool))\n",
    "        \n",
    "        if len(user_neg_pool) >= uneg:\n",
    "            user_neg_samples = user_neg_pool[:uneg]\n",
    "        else:\n",
    "            user_neg_samples = user_neg_pool + [(-1,'')] * (uneg-len(user_neg_pool))\n",
    "        \n",
    "        # sample for item\n",
    "        if len(item_pos_pool) >= ipos:\n",
    "            item_pos_samples = item_pos_pool[:ipos]\n",
    "        else:\n",
    "            item_pos_samples = item_pos_pool + [(-1,'')] * (ipos-len(item_pos_pool))\n",
    "        \n",
    "        if len(item_neg_pool) >= ineg:\n",
    "            item_neg_samples = item_neg_pool[:ineg]\n",
    "        else:\n",
    "            item_neg_samples = item_neg_pool + [(-1,'')] * (ineg-len(item_neg_pool))\n",
    "        \n",
    "        # prepare for writing file\n",
    "        user_pos_text = '\\t'.join([up[1] for up in user_pos_samples])\n",
    "        user_pos_neighbor = '\\t'.join([str(item_id2idx[up[0]]) if up[0] != -1 else str(-1) for up in user_pos_samples])\n",
    "        user_neg_text = '\\t'.join([un[1] for un in user_neg_samples])\n",
    "        user_neg_neighbor = '\\t'.join([str(item_id2idx[un[0]]) if un[0] != -1 else str(-1) for un in user_neg_samples])\n",
    "        \n",
    "        item_pos_text = '\\t'.join([ip[1] for ip in item_pos_samples])\n",
    "        item_pos_neighbor = '\\t'.join([str(user_id2idx[ip[0]]) if ip[0] != -1 else str(-1) for ip in item_pos_samples])\n",
    "        item_neg_text = '\\t'.join([inn[1] for inn in item_neg_samples])\n",
    "        item_neg_neighbor = '\\t'.join([str(user_id2idx[inn[0]]) if inn[0] != -1 else str(-1) for inn in item_neg_samples])\n",
    "\n",
    "        user_line = str(user_id2idx[d[1][0]]) + '\\*\\*' + user_pos_text + '\\*\\*' + user_neg_text + '\\*\\*' + user_pos_neighbor + '\\*\\*' + user_neg_neighbor\n",
    "        item_line = str(item_id2idx[d[0]]) + '\\*\\*' + item_pos_text + '\\*\\*' + item_neg_text + '\\*\\*' + item_pos_neighbor + '\\*\\*' + item_neg_neighbor\n",
    "        \n",
    "        #fout.write(user_line+'\\$\\$'+item_line+'\\$\\$'+str(d[1][3])+'\\n')\n",
    "        fout.write(item_line+'\\$\\$'+user_line+'\\$\\$'+str(1)+'\\n')\n",
    "        \n",
    "print(f'Number of Valid Test Edges:{valid_test_edges} | Total:{len(test_tuples)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "22662de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save side files\n",
    "\n",
    "pickle.dump([ipos,ineg,upos,uneg],open(f'{output_dir}/{dataset}/neighbor_sampling.pkl','wb'))\n",
    "pickle.dump(user_id2idx,open(f'{output_dir}/{dataset}/user_id2idx.pkl','wb'))\n",
    "pickle.dump(item_id2idx,open(f'{output_dir}/{dataset}/item_id2idx.pkl','wb'))\n",
    "pickle.dump([len(item_id2idx),len(user_id2idx),2],open(f'{output_dir}/{dataset}/node_num.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f2092165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save neighbor file\n",
    "\n",
    "pickle.dump(train_user_neighbor,open(f'{output_dir}/{dataset}/neighbor/train_user_pos_neighbor.pkl','wb'))\n",
    "pickle.dump([],open(f'{output_dir}/{dataset}/neighbor/train_user_neg_neighbor.pkl','wb'))\n",
    "pickle.dump(train_item_neighbor,open(f'{output_dir}/{dataset}/neighbor/train_item_pos_neighbor.pkl','wb'))\n",
    "pickle.dump([],open(f'{output_dir}/{dataset}/neighbor/train_item_neg_neighbor.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ed79ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
